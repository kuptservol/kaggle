{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model, configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path('./deeppavlov-sentiment-twitter-nltk-analyis-ru.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.15.2 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (1.15.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.11.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.28.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.33.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.18.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.2.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (0.8.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (3.11.3)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.15.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.2) (1.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (46.1.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (0.14.1)\n",
      "Requirement already satisfied: h5py in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (0.0.0)\n",
      "Requirement already satisfied: zipp>=0.3.2 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (0.3.3)\n",
      "Requirement already satisfied: fasttext==0.9.1 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from fasttext==0.9.1) (2.5.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from fasttext==0.9.1) (46.1.3)\n",
      "Requirement already satisfied: numpy in /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages (from fasttext==0.9.1) (1.18.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m deeppavlov install ./deeppavlov-sentiment-twitter-nltk-analyis-ru.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/deeppavlov/__main__.py\", line 4, in <module>\r\n",
      "    main()\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/deeppavlov/deep.py\", line 80, in main\r\n",
      "    deep_download(pipeline_config_path)\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/deeppavlov/download.py\", line 147, in deep_download\r\n",
      "    download_resource(url, dest_paths)\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/deeppavlov/download.py\", line 116, in download_resource\r\n",
      "    if check_md5(url, dest_paths):\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/deeppavlov/download.py\", line 97, in check_md5\r\n",
      "    if all(file_md5(base_path / p) == _md5 for p, _md5 in expected.items()):\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/deeppavlov/download.py\", line 97, in <genexpr>\r\n",
      "    if all(file_md5(base_path / p) == _md5 for p, _md5 in expected.items()):\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/deeppavlov/core/data/utils.py\", line 303, in file_md5\r\n",
      "    for chunk in iter(lambda: f.read(chunk_size), b\"\"):\r\n",
      "  File \"/home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/deeppavlov/core/data/utils.py\", line 303, in <lambda>\r\n",
      "    for chunk in iter(lambda: f.read(chunk_size), b\"\"):\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python -m deeppavlov download ./deeppavlov-sentiment-twitter-nltk-analyis-ru.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  {\r\n",
      "    \"text\": \"Досудебное расследование по факту покупки ЕНПФ пакета облигаций ТОО \\\"Бузгул Аурум\\\" было начато по инициативе Национального банка РК, сообщил директор департамента защиты прав потребителей и финансовых услуг Нацбанка Казахстана Александр Терентьев.\\n\\\"Основанием для досудебного расследования стало обращение Национального банка, письмо от 25 ноября 2016 года. Было обращение Национального банка в правоохранительные органы. Нам эта сделка показалась сомнительной, недостаточно корректной, поэтому Нацбанк 25 ноября 2016 года обратился в правоохранительные органы. Это то, что я могу озвучить на сегодня. Идёт следствие, проводится проверка\\\", – сказал Терентьев.\\n28 декабря в Нацбанке заявили, что не знают, что стало основанием для проверки ЕНПФ.\\n23 декабря факт проведения проверки в АО \\\"Единый накопительный пенсионный фонд\\\" подтвердился. Пресс-служба Национального банка сообщила, что проверку проводят по операциям, совершённым АО \\\"ЕНПФ\\\" в отношении инвестирования собственных активов.\\nТакже в финрегуляторе сообщали, что по сделке ЕНПФ на сумму в пять млрд заведено уголовное дело. В Нацбанке заверяют, что всё происходящее не затрагивает пенсионных накоплений казахстанцев.\\nЕсли вы нашли ошибку в тексте, выделите ее мышью и нажмите Ctrl+Enter\\n\", \r\n",
      "    \"id\": 1945, \r\n",
      "    \"sentiment\": 0\r\n",
      "  }, \r\n",
      "  {\r\n",
      "    \"text\": \"Медики рассказали о состоянии пострадавшего мужчины, на которого было совершено нападение возле отделения банка по Тимирязева. Как прокомментировали Tengrinews.kz в пресс-службе Управления здравоохранения Алматы, с места происшествия в службу скорой помощи обратились двое человек. \\n\\n«Одному из них на месте была оказана медицинская помощь. От госпитализации он отказался. Второй пациент был доставлен в больницу скорой неотложной помощи (БСНП) с сотрясением головного мозга, ушибленной раной головы. Состояние на данный момент оценивается ближе к удовлетворительному. Пока он проходит обследование в больнице», — сообщили в Управлении здравоохранения Алматы.  \\n\\nНапомним, в Алматы на пересечении улиц Тимирязева и Маркова возле БЦ «Алатау Гранд» произошла стрельба, ориентировочно в обеденное время. В здании расположены отделения банков «ВТБ» и «Сбербанк». \\n\\nВ настоящее время полицейские разыскивают подозреваемых в стрельбе. По факту нападения в местном управлении внутренних дел начато досудебное расследование по статье 192 УК РК «Разбой». Создана специальная следственно-оперативная группа из числа опытных сотрудников подразделений криминальной полиции. В настоящий момент проводится комплекс оперативных и следственных мероприятий, направленных на установление личностей нападавших и их задержание. \\n\\nРанее в «ДО АО Банк ВТБ (Казахстан)» прокомментировали нападение на мужчину. По данным пресс-службы банка, все отделения банка работают в штатном режиме с применением усиленных мер безопасности.   сотрудники полиции работают внутри отделения банка «ВТБ». Место происшествия не оцеплено. \\n\\n \\n\", \r\n",
      "    \"id\": 1957, \r\n",
      "    \"sentiment\": 0\r\n"
     ]
    }
   ],
   "source": [
    "! head ~/.kaggle/datasets/ru-sentiment/train_proc.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = pd.read_json('~/.kaggle/datasets/ru-sentiment/train.json');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Досудебное расследование по факту покупки ЕНПФ...</td>\n",
       "      <td>1945</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Медики рассказали о состоянии пострадавшего му...</td>\n",
       "      <td>1957</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Прошел почти год, как железнодорожным оператор...</td>\n",
       "      <td>1969</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>По итогам 12 месяцев 2016 года на территории р...</td>\n",
       "      <td>1973</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Астана. 21 ноября. Kazakhstan Today - Агентств...</td>\n",
       "      <td>1975</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    id sentiment\n",
       "0  Досудебное расследование по факту покупки ЕНПФ...  1945  negative\n",
       "1  Медики рассказали о состоянии пострадавшего му...  1957  negative\n",
       "2  Прошел почти год, как железнодорожным оператор...  1969  negative\n",
       "3  По итогам 12 месяцев 2016 года на территории р...  1973  negative\n",
       "4  Астана. 21 ноября. Kazakhstan Today - Агентств...  1975  negative"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[\"sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8263"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(row['text'], str(row['sentiment'])) for _, row in trn.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 15:29:53.382 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 109: Cannot find /home/kuptservol/.kaggle/datasets/ru-sentiment/valid.json file\n",
      "2020-05-11 15:29:53.383 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 109: Cannot find /home/kuptservol/.kaggle/datasets/ru-sentiment/test.json file\n",
      "2020-05-11 15:29:53.385 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 74: Splitting field <<train>> to new fields <<['train', 'valid']>>\n",
      "2020-05-11 15:29:53.390 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/classes.dict]\n",
      "2020-05-11 15:29:53.397 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/classes.dict]\n",
      "2020-05-11 15:29:53.398 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 53: [loading fastText embeddings from `/home/kuptservol/.deeppavlov/downloads/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.bin`]\n",
      "\n",
      "2020-05-11 15:30:06.139 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 216: [initializing `KerasClassificationModel` from scratch as cnn_model]\n",
      "2020-05-11 15:30:06.846 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 129: Model was successfully initialized!\n",
      "Model summary:\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 300)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 256)    230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 256)    384256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 256)    537856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 256)    1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 256)    1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 256)    1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 768)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          76900       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100)          400         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            303         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 3)            12          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 3)            0           batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,233,455\n",
      "Trainable params: 1,231,713\n",
      "Non-trainable params: 1,742\n",
      "__________________________________________________________________________________________________\n",
      "2020-05-11 15:32:04.525 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 198: Initial best accuracy of 0.4933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.4933, \"f1_macro\": 0.2321, \"roc_auc\": 0.5158}, \"time_spent\": \"0:01:58\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "WARNING:tensorflow:From /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.5312, \"f1_macro\": 0.3222, \"roc_auc\": 0.7725}, \"time_spent\": \"0:32:17\", \"epochs_done\": 1, \"batches_seen\": 117, \"train_examples_seen\": 7436, \"loss\": 1.7211972317125044}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 16:04:15.207 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best accuracy of 0.5357\n",
      "2020-05-11 16:04:15.208 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2020-05-11 16:04:15.209 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 346: [saving model to /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/model_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.5357, \"f1_macro\": 0.3262, \"roc_auc\": 0.797}, \"time_spent\": \"0:34:09\", \"epochs_done\": 1, \"batches_seen\": 117, \"train_examples_seen\": 7436, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.4531, \"f1_macro\": 0.3251, \"roc_auc\": 0.9014}, \"time_spent\": \"1:03:31\", \"epochs_done\": 2, \"batches_seen\": 234, \"train_examples_seen\": 14872, \"loss\": 0.7973781396181155}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 16:35:27.542 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best accuracy of 0.5405\n",
      "2020-05-11 16:35:27.543 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2020-05-11 16:35:27.544 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 346: [saving model to /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/model_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.5405, \"f1_macro\": 0.3443, \"roc_auc\": 0.8317}, \"time_spent\": \"1:05:21\", \"epochs_done\": 2, \"batches_seen\": 234, \"train_examples_seen\": 14872, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.7344, \"f1_macro\": 0.7098, \"roc_auc\": 0.9227}, \"time_spent\": \"1:35:07\", \"epochs_done\": 3, \"batches_seen\": 351, \"train_examples_seen\": 22308, \"loss\": 0.6753407732034341}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 17:07:05.58 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best accuracy of 0.6663\n",
      "2020-05-11 17:07:05.59 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2020-05-11 17:07:05.60 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 346: [saving model to /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/model_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.6663, \"f1_macro\": 0.6145, \"roc_auc\": 0.8448}, \"time_spent\": \"1:36:59\", \"epochs_done\": 3, \"batches_seen\": 351, \"train_examples_seen\": 22308, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.7969, \"f1_macro\": 0.7272, \"roc_auc\": 0.928}, \"time_spent\": \"2:07:29\", \"epochs_done\": 4, \"batches_seen\": 468, \"train_examples_seen\": 29744, \"loss\": 0.6169733547756815}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 17:39:27.859 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best accuracy of 0.711\n",
      "2020-05-11 17:39:27.859 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2020-05-11 17:39:27.860 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 346: [saving model to /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/model_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.711, \"f1_macro\": 0.6931, \"roc_auc\": 0.8571}, \"time_spent\": \"2:09:22\", \"epochs_done\": 4, \"batches_seen\": 468, \"train_examples_seen\": 29744, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.7969, \"f1_macro\": 0.7626, \"roc_auc\": 0.9249}, \"time_spent\": \"2:39:54\", \"epochs_done\": 5, \"batches_seen\": 585, \"train_examples_seen\": 37180, \"loss\": 0.5637738100993328}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 18:11:51.332 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the accuracy of 0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.7074, \"f1_macro\": 0.6916, \"roc_auc\": 0.8618}, \"time_spent\": \"2:41:45\", \"epochs_done\": 5, \"batches_seen\": 585, \"train_examples_seen\": 37180, \"impatience\": 1, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.8281, \"f1_macro\": 0.8047, \"roc_auc\": 0.9556}, \"time_spent\": \"3:11:32\", \"epochs_done\": 6, \"batches_seen\": 702, \"train_examples_seen\": 44616, \"loss\": 0.5334965102692955}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 18:43:27.668 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the accuracy of 0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.7074, \"f1_macro\": 0.6939, \"roc_auc\": 0.8639}, \"time_spent\": \"3:13:21\", \"epochs_done\": 6, \"batches_seen\": 702, \"train_examples_seen\": 44616, \"impatience\": 2, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.8906, \"f1_macro\": 0.9166, \"roc_auc\": 0.9781}, \"time_spent\": \"3:43:31\", \"epochs_done\": 7, \"batches_seen\": 819, \"train_examples_seen\": 52052, \"loss\": 0.4982842018461635}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 19:15:27.487 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the accuracy of 0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.7098, \"f1_macro\": 0.7054, \"roc_auc\": 0.8676}, \"time_spent\": \"3:45:21\", \"epochs_done\": 7, \"batches_seen\": 819, \"train_examples_seen\": 52052, \"impatience\": 3, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.9219, \"f1_macro\": 0.9232, \"roc_auc\": 0.9901}, \"time_spent\": \"4:14:52\", \"epochs_done\": 8, \"batches_seen\": 936, \"train_examples_seen\": 59488, \"loss\": 0.4750671407096406}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 19:46:49.603 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best accuracy of 0.7267\n",
      "2020-05-11 19:46:49.604 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2020-05-11 19:46:49.604 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 346: [saving model to /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/model_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.7267, \"f1_macro\": 0.7151, \"roc_auc\": 0.8687}, \"time_spent\": \"4:16:43\", \"epochs_done\": 8, \"batches_seen\": 936, \"train_examples_seen\": 59488, \"impatience\": 0, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.9219, \"f1_macro\": 0.9144, \"roc_auc\": 0.9891}, \"time_spent\": \"4:47:14\", \"epochs_done\": 9, \"batches_seen\": 1053, \"train_examples_seen\": 66924, \"loss\": 0.4490912847029857}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 20:19:11.859 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 211: Did not improve on the accuracy of 0.7267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.7231, \"f1_macro\": 0.7147, \"roc_auc\": 0.8661}, \"time_spent\": \"4:49:06\", \"epochs_done\": 9, \"batches_seen\": 1053, \"train_examples_seen\": 66924, \"impatience\": 1, \"patience_limit\": 5}}\n",
      "{\"train\": {\"eval_examples_count\": 64, \"metrics\": {\"accuracy\": 0.9062, \"f1_macro\": 0.9154, \"roc_auc\": 0.9897}, \"time_spent\": \"5:18:39\", \"epochs_done\": 10, \"batches_seen\": 1170, \"train_examples_seen\": 74360, \"loss\": 0.42946480558468747}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 20:50:36.458 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best accuracy of 0.7304\n",
      "2020-05-11 20:50:36.459 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n",
      "2020-05-11 20:50:36.459 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 346: [saving model to /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/model_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.7304, \"f1_macro\": 0.7242, \"roc_auc\": 0.8699}, \"time_spent\": \"5:20:30\", \"epochs_done\": 10, \"batches_seen\": 1170, \"train_examples_seen\": 74360, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 20:50:36.981 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/classes.dict]\n",
      "2020-05-11 20:50:36.988 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 53: [loading fastText embeddings from `/home/kuptservol/.deeppavlov/downloads/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.bin`]\n",
      "\n",
      "2020-05-11 20:50:46.168 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 245: [initializing `KerasClassificationModel` from saved]\n",
      "2020-05-11 20:50:46.653 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 255: [loading weights from model.h5]\n",
      "2020-05-11 20:50:47.48 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 129: Model was successfully initialized!\n",
      "Model summary:\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 300)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 256)    230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 256)    384256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 256)    537856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 256)    1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 256)    1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 256)    1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 768)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          76900       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100)          400         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            303         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 3)            12          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 3)            0           batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,233,455\n",
      "Trainable params: 1,231,713\n",
      "Non-trainable params: 1,742\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 7436, \"metrics\": {\"accuracy\": 0.912, \"f1_macro\": 0.9116, \"roc_auc\": 0.9863}, \"time_spent\": \"0:13:20\"}}\n",
      "{\"valid\": {\"eval_examples_count\": 827, \"metrics\": {\"accuracy\": 0.7304, \"f1_macro\": 0.7242, \"roc_auc\": 0.8699}, \"time_spent\": \"0:02:05\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 21:06:11.665 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/classes.dict]\n",
      "2020-05-11 21:06:11.671 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 53: [loading fastText embeddings from `/home/kuptservol/.deeppavlov/downloads/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.bin`]\n",
      "\n",
      "2020-05-11 21:06:22.229 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 245: [initializing `KerasClassificationModel` from saved]\n",
      "2020-05-11 21:06:22.854 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 255: [loading weights from model.h5]\n",
      "2020-05-11 21:06:23.347 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 129: Model was successfully initialized!\n",
      "Model summary:\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 300)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 256)    230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 256)    384256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 256)    537856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 256)    1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 256)    1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 256)    1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 768)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          76900       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100)          400         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            303         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 3)            12          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 3)            0           batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,233,455\n",
      "Trainable params: 1,231,713\n",
      "Non-trainable params: 1,742\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = train_model(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-12 06:23:56.867 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /home/kuptservol/.deeppavlov/models/classifiers/deeppavlov-sentiment-twitter-nltk-analyis-ru/classes.dict]\n",
      "[nltk_data] Downloading package punkt to /home/kuptservol/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kuptservol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/kuptservol/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/kuptservol/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "2020-05-12 06:23:57.809 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 53: [loading fastText embeddings from `/home/kuptservol/.deeppavlov/downloads/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.bin`]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_backend.py:38: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-12 06:24:08.191 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 245: [initializing `KerasClassificationModel` from saved]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kuptservol/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-12 06:24:08.667 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 255: [loading weights from model.h5]\n",
      "2020-05-12 06:24:09.8 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 129: Model was successfully initialized!\n",
      "Model summary:\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 300)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 256)    230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 256)    384256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 256)    537856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 256)    1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 256)    1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 256)    1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 768)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          76900       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100)          400         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            303         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 3)            12          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 3)            0           batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,233,455\n",
      "Trainable params: 1,231,713\n",
      "Non-trainable params: 1,742\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, sentiment]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(columns=['id', 'sentiment']); submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('~/.kaggle/datasets/ru-sentiment/_test.json');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Как сообщает пресс-служба акимата Алматы, для ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Казахстанские авиакомпании перевозят 250 тысяч...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>На состоявшемся под председательством Касым-Жо...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В ОАЭ состоялись переговоры между казахстанско...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 вагонов грузового поезда сошли с путей в Во...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  id\n",
       "0  Как сообщает пресс-служба акимата Алматы, для ...   0\n",
       "1  Казахстанские авиакомпании перевозят 250 тысяч...   1\n",
       "2  На состоявшемся под председательством Касым-Жо...   2\n",
       "3  В ОАЭ состоялись переговоры между казахстанско...   3\n",
       "4  12 вагонов грузового поезда сошли с путей в Во...   4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id'] = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      neutral\n",
       "1     positive\n",
       "2     negative\n",
       "3     positive\n",
       "4     negative\n",
       "5      neutral\n",
       "6     positive\n",
       "7     positive\n",
       "8      neutral\n",
       "9      neutral\n",
       "10    positive\n",
       "11    positive\n",
       "12    negative\n",
       "13     neutral\n",
       "14     neutral\n",
       "15    negative\n",
       "16    positive\n",
       "17     neutral\n",
       "18     neutral\n",
       "19    positive\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'][:20].apply(lambda x: model([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['sentiment'] = test['text'].apply(lambda x: model([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id sentiment\n",
       "0   0   neutral\n",
       "1   1  positive\n",
       "2   2  negative\n",
       "3   3  positive\n",
       "4   4  negative"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 26.0k/26.0k [00:07<00:00, 3.59kB/s]\n",
      "Successfully submitted to Sentiment Analysis in Russian"
     ]
    }
   ],
   "source": [
    "! kaggle competitions submit sentiment-analysis-in-russian -f {'./submission.csv'} -m \"submission\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
