{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y) * sigmoid_prime(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "        \n",
    "    def default_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x) \n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "    def large_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print \"Epoch %s training complete\" % j\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print \"Cost on training data: {}\".format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print \"Accuracy on training data: {} / {} = {} %\".format(\n",
    "                    accuracy, n, float(accuracy)*100/n)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print \"Cost on evaluation data: {}\".format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print \"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data)\n",
    "            print\n",
    "        print \"Max accuracy on training data : {} \".format(float(training_accuracy[np.argmax(training_accuracy)])*100/n)\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy, \n",
    "                \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def accuracy(self, data, convert=False):\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def load_data(path, from_= 0):\n",
    "    results = []\n",
    "    inputs = []\n",
    "    with open(path) as test_data_file:\n",
    "        reader = csv.reader(test_data_file)\n",
    "        next(reader)\n",
    "\n",
    "        for row in reader:\n",
    "            row = map(int, row)\n",
    "            \n",
    "            result = vectorized_result(row[0])\n",
    "\n",
    "            input = np.array(row[from_:])\n",
    "            input = np.reshape(input, (784, 1))\n",
    "\n",
    "            inputs.append(input)\n",
    "            results.append(result)\n",
    "\n",
    "    data = zip(inputs, results)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_data('./data/train.csv', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 100, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuptservol/.pyenv/versions/2.7.12/lib/python2.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 0.969787833801\n",
      "Accuracy on training data: 37193 / 42000 = 88.5547619048 %\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.736549726984\n",
      "Accuracy on training data: 38010 / 42000 = 90.5 %\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 0.700123485087\n",
      "Accuracy on training data: 38112 / 42000 = 90.7428571429 %\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.646662012775\n",
      "Accuracy on training data: 38464 / 42000 = 91.580952381 %\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.594270943328\n",
      "Accuracy on training data: 38590 / 42000 = 91.880952381 %\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.552572874667\n",
      "Accuracy on training data: 38776 / 42000 = 92.3238095238 %\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.550816034182\n",
      "Accuracy on training data: 38682 / 42000 = 92.1 %\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.551707286649\n",
      "Accuracy on training data: 38883 / 42000 = 92.5785714286 %\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.502539252268\n",
      "Accuracy on training data: 39011 / 42000 = 92.8833333333 %\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.511230324762\n",
      "Accuracy on training data: 39052 / 42000 = 92.980952381 %\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.511948248658\n",
      "Accuracy on training data: 39071 / 42000 = 93.0261904762 %\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.504302775034\n",
      "Accuracy on training data: 39079 / 42000 = 93.0452380952 %\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.469619467212\n",
      "Accuracy on training data: 39267 / 42000 = 93.4928571429 %\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.486196136949\n",
      "Accuracy on training data: 39156 / 42000 = 93.2285714286 %\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.463916849987\n",
      "Accuracy on training data: 39300 / 42000 = 93.5714285714 %\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.446985543108\n",
      "Accuracy on training data: 39386 / 42000 = 93.7761904762 %\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.438414427137\n",
      "Accuracy on training data: 39487 / 42000 = 94.0166666667 %\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.43533958738\n",
      "Accuracy on training data: 39465 / 42000 = 93.9642857143 %\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.439950428222\n",
      "Accuracy on training data: 39347 / 42000 = 93.6833333333 %\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.446716047607\n",
      "Accuracy on training data: 39381 / 42000 = 93.7642857143 %\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.443319578775\n",
      "Accuracy on training data: 39370 / 42000 = 93.7380952381 %\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.436718466849\n",
      "Accuracy on training data: 39423 / 42000 = 93.8642857143 %\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.422006310102\n",
      "Accuracy on training data: 39545 / 42000 = 94.1547619048 %\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.422689845201\n",
      "Accuracy on training data: 39596 / 42000 = 94.2761904762 %\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.396807136339\n",
      "Accuracy on training data: 39675 / 42000 = 94.4642857143 %\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.414314212023\n",
      "Accuracy on training data: 39496 / 42000 = 94.0380952381 %\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.408437470047\n",
      "Accuracy on training data: 39649 / 42000 = 94.4023809524 %\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.412659169097\n",
      "Accuracy on training data: 39609 / 42000 = 94.3071428571 %\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.3871624837\n",
      "Accuracy on training data: 39777 / 42000 = 94.7071428571 %\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.373740950039\n",
      "Accuracy on training data: 39820 / 42000 = 94.8095238095 %\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.378008780519\n",
      "Accuracy on training data: 39786 / 42000 = 94.7285714286 %\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.377078470872\n",
      "Accuracy on training data: 39834 / 42000 = 94.8428571429 %\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.402552794391\n",
      "Accuracy on training data: 39710 / 42000 = 94.5476190476 %\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.38417620775\n",
      "Accuracy on training data: 39839 / 42000 = 94.8547619048 %\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.369226850791\n",
      "Accuracy on training data: 39886 / 42000 = 94.9666666667 %\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.373872913158\n",
      "Accuracy on training data: 39889 / 42000 = 94.9738095238 %\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.390979165636\n",
      "Accuracy on training data: 39717 / 42000 = 94.5642857143 %\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.362941665246\n",
      "Accuracy on training data: 39892 / 42000 = 94.980952381 %\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.372091120251\n",
      "Accuracy on training data: 39925 / 42000 = 95.0595238095 %\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.362425818442\n",
      "Accuracy on training data: 39880 / 42000 = 94.9523809524 %\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.362555345657\n",
      "Accuracy on training data: 39882 / 42000 = 94.9571428571 %\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.358932604077\n",
      "Accuracy on training data: 39929 / 42000 = 95.069047619 %\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.365192903262\n",
      "Accuracy on training data: 39901 / 42000 = 95.0023809524 %\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.354053821254\n",
      "Accuracy on training data: 39950 / 42000 = 95.119047619 %\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.364578518102\n",
      "Accuracy on training data: 39883 / 42000 = 94.9595238095 %\n",
      "\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.354703850023\n",
      "Accuracy on training data: 39933 / 42000 = 95.0785714286 %\n",
      "\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.34410673172\n",
      "Accuracy on training data: 40030 / 42000 = 95.3095238095 %\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.372388609614\n",
      "Accuracy on training data: 39877 / 42000 = 94.9452380952 %\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.342927749966\n",
      "Accuracy on training data: 40054 / 42000 = 95.3666666667 %\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.357376265283\n",
      "Accuracy on training data: 39916 / 42000 = 95.0380952381 %\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.360175028968\n",
      "Accuracy on training data: 39917 / 42000 = 95.0404761905 %\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.36231448261\n",
      "Accuracy on training data: 39875 / 42000 = 94.9404761905 %\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.350110223589\n",
      "Accuracy on training data: 40012 / 42000 = 95.2666666667 %\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.357114075961\n",
      "Accuracy on training data: 39990 / 42000 = 95.2142857143 %\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.361926586083\n",
      "Accuracy on training data: 39910 / 42000 = 95.0238095238 %\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.357280093492\n",
      "Accuracy on training data: 39961 / 42000 = 95.1452380952 %\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.346163036248\n",
      "Accuracy on training data: 39995 / 42000 = 95.2261904762 %\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.336398590774\n",
      "Accuracy on training data: 40026 / 42000 = 95.3 %\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.341129272994\n",
      "Accuracy on training data: 40068 / 42000 = 95.4 %\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.346913265289\n",
      "Accuracy on training data: 40034 / 42000 = 95.319047619 %\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.321202534755\n",
      "Accuracy on training data: 40155 / 42000 = 95.6071428571 %\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.328436881113\n",
      "Accuracy on training data: 40089 / 42000 = 95.45 %\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.3189094026\n",
      "Accuracy on training data: 40247 / 42000 = 95.8261904762 %\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.326671791062\n",
      "Accuracy on training data: 40117 / 42000 = 95.5166666667 %\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.320175210956\n",
      "Accuracy on training data: 40186 / 42000 = 95.680952381 %\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.352824118206\n",
      "Accuracy on training data: 39997 / 42000 = 95.230952381 %\n",
      "\n",
      "Epoch 66 training complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on training data: 0.341236427123\n",
      "Accuracy on training data: 40062 / 42000 = 95.3857142857 %\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.336289764574\n",
      "Accuracy on training data: 40095 / 42000 = 95.4642857143 %\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.332495584245\n",
      "Accuracy on training data: 40146 / 42000 = 95.5857142857 %\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.33613482535\n",
      "Accuracy on training data: 40128 / 42000 = 95.5428571429 %\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.336134198676\n",
      "Accuracy on training data: 40115 / 42000 = 95.5119047619 %\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.32377074681\n",
      "Accuracy on training data: 40211 / 42000 = 95.7404761905 %\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.32601634443\n",
      "Accuracy on training data: 40243 / 42000 = 95.8166666667 %\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.316081490275\n",
      "Accuracy on training data: 40184 / 42000 = 95.6761904762 %\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.331521687739\n",
      "Accuracy on training data: 40121 / 42000 = 95.5261904762 %\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.30316243181\n",
      "Accuracy on training data: 40302 / 42000 = 95.9571428571 %\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.314565228336\n",
      "Accuracy on training data: 40225 / 42000 = 95.7738095238 %\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.318031952484\n",
      "Accuracy on training data: 40171 / 42000 = 95.6452380952 %\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.303791096855\n",
      "Accuracy on training data: 40273 / 42000 = 95.8880952381 %\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.321415485571\n",
      "Accuracy on training data: 40190 / 42000 = 95.6904761905 %\n",
      "\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.335180494486\n",
      "Accuracy on training data: 40112 / 42000 = 95.5047619048 %\n",
      "\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.323195746626\n",
      "Accuracy on training data: 40212 / 42000 = 95.7428571429 %\n",
      "\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.322055835841\n",
      "Accuracy on training data: 40203 / 42000 = 95.7214285714 %\n",
      "\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.308903172641\n",
      "Accuracy on training data: 40248 / 42000 = 95.8285714286 %\n",
      "\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.31678377727\n",
      "Accuracy on training data: 40187 / 42000 = 95.6833333333 %\n",
      "\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.334818207965\n",
      "Accuracy on training data: 40113 / 42000 = 95.5071428571 %\n",
      "\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.314537537749\n",
      "Accuracy on training data: 40208 / 42000 = 95.7333333333 %\n",
      "\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.334781177302\n",
      "Accuracy on training data: 40100 / 42000 = 95.4761904762 %\n",
      "\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.319054412127\n",
      "Accuracy on training data: 40201 / 42000 = 95.7166666667 %\n",
      "\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.314449296106\n",
      "Accuracy on training data: 40193 / 42000 = 95.6976190476 %\n",
      "\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.312090356809\n",
      "Accuracy on training data: 40197 / 42000 = 95.7071428571 %\n",
      "\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.311651409449\n",
      "Accuracy on training data: 40206 / 42000 = 95.7285714286 %\n",
      "\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.304310063864\n",
      "Accuracy on training data: 40282 / 42000 = 95.9095238095 %\n",
      "\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.297984146703\n",
      "Accuracy on training data: 40311 / 42000 = 95.9785714286 %\n",
      "\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.308825567637\n",
      "Accuracy on training data: 40279 / 42000 = 95.9023809524 %\n",
      "\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.308910041129\n",
      "Accuracy on training data: 40294 / 42000 = 95.9380952381 %\n",
      "\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.301189450877\n",
      "Accuracy on training data: 40322 / 42000 = 96.0047619048 %\n",
      "\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.319570794924\n",
      "Accuracy on training data: 40241 / 42000 = 95.8119047619 %\n",
      "\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.323820204437\n",
      "Accuracy on training data: 40178 / 42000 = 95.6619047619 %\n",
      "\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.309083487623\n",
      "Accuracy on training data: 40259 / 42000 = 95.8547619048 %\n",
      "\n",
      "Max accuracy on training data : 96.0047619048 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [],\n",
       " [0.9697878338007592,\n",
       "  0.7365497269844945,\n",
       "  0.7001234850874752,\n",
       "  0.6466620127752469,\n",
       "  0.5942709433281335,\n",
       "  0.5525728746671702,\n",
       "  0.5508160341818499,\n",
       "  0.5517072866485305,\n",
       "  0.5025392522679404,\n",
       "  0.5112303247617035,\n",
       "  0.5119482486577766,\n",
       "  0.5043027750342501,\n",
       "  0.4696194672115899,\n",
       "  0.48619613694933733,\n",
       "  0.4639168499873496,\n",
       "  0.4469855431084868,\n",
       "  0.43841442713737955,\n",
       "  0.43533958738035755,\n",
       "  0.43995042822159375,\n",
       "  0.44671604760696915,\n",
       "  0.44331957877542094,\n",
       "  0.43671846684863025,\n",
       "  0.42200631010222883,\n",
       "  0.4226898452005009,\n",
       "  0.39680713633923786,\n",
       "  0.4143142120226303,\n",
       "  0.40843747004727937,\n",
       "  0.41265916909696787,\n",
       "  0.387162483699972,\n",
       "  0.37374095003930713,\n",
       "  0.3780087805190951,\n",
       "  0.3770784708722602,\n",
       "  0.4025527943907117,\n",
       "  0.3841762077501722,\n",
       "  0.3692268507906754,\n",
       "  0.3738729131576916,\n",
       "  0.39097916563552243,\n",
       "  0.36294166524620836,\n",
       "  0.3720911202506947,\n",
       "  0.3624258184422081,\n",
       "  0.36255534565736336,\n",
       "  0.3589326040767418,\n",
       "  0.36519290326151843,\n",
       "  0.35405382125366985,\n",
       "  0.36457851810182507,\n",
       "  0.3547038500231674,\n",
       "  0.3441067317200554,\n",
       "  0.3723886096143356,\n",
       "  0.34292774996635744,\n",
       "  0.3573762652834681,\n",
       "  0.36017502896759374,\n",
       "  0.3623144826099244,\n",
       "  0.350110223589401,\n",
       "  0.3571140759611151,\n",
       "  0.36192658608339084,\n",
       "  0.3572800934918429,\n",
       "  0.34616303624779543,\n",
       "  0.33639859077373774,\n",
       "  0.3411292729938947,\n",
       "  0.3469132652886754,\n",
       "  0.32120253475518806,\n",
       "  0.3284368811125286,\n",
       "  0.3189094025999426,\n",
       "  0.3266717910619779,\n",
       "  0.32017521095609475,\n",
       "  0.3528241182061007,\n",
       "  0.3412364271231244,\n",
       "  0.3362897645738301,\n",
       "  0.33249558424532083,\n",
       "  0.3361348253499153,\n",
       "  0.33613419867647787,\n",
       "  0.3237707468097081,\n",
       "  0.3260163444304521,\n",
       "  0.3160814902754741,\n",
       "  0.3315216877388645,\n",
       "  0.303162431809895,\n",
       "  0.3145652283355367,\n",
       "  0.3180319524836901,\n",
       "  0.3037910968554936,\n",
       "  0.321415485570867,\n",
       "  0.33518049448644216,\n",
       "  0.323195746625752,\n",
       "  0.3220558358411258,\n",
       "  0.3089031726411087,\n",
       "  0.31678377726974355,\n",
       "  0.3348182079651948,\n",
       "  0.3145375377490053,\n",
       "  0.3347811773024902,\n",
       "  0.3190544121269092,\n",
       "  0.3144492961057114,\n",
       "  0.3120903568086939,\n",
       "  0.31165140944910374,\n",
       "  0.3043100638640053,\n",
       "  0.2979841467034099,\n",
       "  0.3088255676370694,\n",
       "  0.30891004112885184,\n",
       "  0.30118945087674087,\n",
       "  0.319570794924442,\n",
       "  0.3238202044365341,\n",
       "  0.30908348762259374],\n",
       " [37193,\n",
       "  38010,\n",
       "  38112,\n",
       "  38464,\n",
       "  38590,\n",
       "  38776,\n",
       "  38682,\n",
       "  38883,\n",
       "  39011,\n",
       "  39052,\n",
       "  39071,\n",
       "  39079,\n",
       "  39267,\n",
       "  39156,\n",
       "  39300,\n",
       "  39386,\n",
       "  39487,\n",
       "  39465,\n",
       "  39347,\n",
       "  39381,\n",
       "  39370,\n",
       "  39423,\n",
       "  39545,\n",
       "  39596,\n",
       "  39675,\n",
       "  39496,\n",
       "  39649,\n",
       "  39609,\n",
       "  39777,\n",
       "  39820,\n",
       "  39786,\n",
       "  39834,\n",
       "  39710,\n",
       "  39839,\n",
       "  39886,\n",
       "  39889,\n",
       "  39717,\n",
       "  39892,\n",
       "  39925,\n",
       "  39880,\n",
       "  39882,\n",
       "  39929,\n",
       "  39901,\n",
       "  39950,\n",
       "  39883,\n",
       "  39933,\n",
       "  40030,\n",
       "  39877,\n",
       "  40054,\n",
       "  39916,\n",
       "  39917,\n",
       "  39875,\n",
       "  40012,\n",
       "  39990,\n",
       "  39910,\n",
       "  39961,\n",
       "  39995,\n",
       "  40026,\n",
       "  40068,\n",
       "  40034,\n",
       "  40155,\n",
       "  40089,\n",
       "  40247,\n",
       "  40117,\n",
       "  40186,\n",
       "  39997,\n",
       "  40062,\n",
       "  40095,\n",
       "  40146,\n",
       "  40128,\n",
       "  40115,\n",
       "  40211,\n",
       "  40243,\n",
       "  40184,\n",
       "  40121,\n",
       "  40302,\n",
       "  40225,\n",
       "  40171,\n",
       "  40273,\n",
       "  40190,\n",
       "  40112,\n",
       "  40212,\n",
       "  40203,\n",
       "  40248,\n",
       "  40187,\n",
       "  40113,\n",
       "  40208,\n",
       "  40100,\n",
       "  40201,\n",
       "  40193,\n",
       "  40197,\n",
       "  40206,\n",
       "  40282,\n",
       "  40311,\n",
       "  40279,\n",
       "  40294,\n",
       "  40322,\n",
       "  40241,\n",
       "  40178,\n",
       "  40259])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.SGD(training_data, 65, 30, 0.01, lmbda = 1.0,\n",
    "            evaluation_data=training_data,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=True,\n",
    "            monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "test_data = load_data('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuptservol/.pyenv/versions/2.7.12/lib/python2.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "with open('data/submission.csv', 'wb') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    csvwriter.writerow(['ImageId', 'Label'])\n",
    "    for i in range(2800):\n",
    "        test_results = net.feedforward(test_data[i][0])\n",
    "        digit_decision =  np.argmax(test_results)\n",
    "        csvwriter.writerow([i+1, digit_decision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
